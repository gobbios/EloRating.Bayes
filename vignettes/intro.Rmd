---
title: "intro"
author: "Christof Neumann"
date: "`r Sys.Date()`"
output: 
  bookdown::pdf_document2:
    toc: true
    toc_depth: 2
    number_sections: false
vignette: >
  %\VignetteIndexEntry{intro}
  %\VignetteEncoding{UTF-8}
  %\VignetteEngine{knitr::rmarkdown}
editor_options: 
  chunk_output_type: console
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup, include=FALSE}
library(EloRating.Bayes)
library(EloRating)
```

```{r setup2, echo=FALSE, include=FALSE}
library(xtable)
```


# technicalities

You need the R package `cmdstanr` to use the functions in the package.


\clearpage

# workflow

## prelims

To apply the Bayesian version of Elo-rating, we need these things in terms of data:

- a vector of winner ids (character or factor)

- a vector of loser ids (character or factor)

- a vector of dates corresponding to the interactions (must be in YYYY-MM-DD format)

- optionally: a presence matrix or data frame (see below for details)

- optionally: a vector (either logical or 0/1) indicating whether an interaction ended in a draw

- optionally: a vector (either character or factor) indicating the type (or intensity) of an interactions

The three vectors for winner, loser, Date (and draws if supplied) must be of the same length.
If you have used the classic `EloRating` package before: this works the same here.

The presence matrix represents for *each **interaction*** whether a given individual was present or not. 
So we require as many rows as interactions, and as many columns as there are individuals.
And this is a major difference to `EloRating` where each row in the presence matrix corresponds to a calender date (regardless of how many interactions occurred that day).

The optional vectors for draws and interaction type also need to be of the same length (and of the same length as winner, loser and Date, obviously).

The following example illustrates this for data set in which interactions were *not* observed on each day between data set start and end (table \ref{tab:tab_seq_toy}). 
The correct version of a presence matrix for usage with `elo_seq_bayes()` is the one in table \ref{tab:tab_new}: each row corresponds to an interaction, i.e. it shows for each interaction the individuals that were present of that interaction's date.
Table \ref{tab:tab_old} shows the version version which would be appropriate for `EloRating`.

```{r, echo=FALSE}
idat <- data.frame(date = c("2000-01-01", "2000-01-03", "2000-01-03", "2000-01-04", "2000-01-06"),
                   winner = c("a", "b", "c", "a", "d"),
                   loser = c("b", "c", "b", "d", "e"))

pmat_old <- matrix(ncol = 5, nrow = 6, 1)
colnames(pmat_old) <- letters[1:5]
pmat_old <- data.frame(Date = c("2000-01-01", "2000-01-02", "2000-01-03", "2000-01-04", "2000-01-05", "2000-01-06"),
                       pmat_old)
pmat_old$e[1:3] <- 0
pmat_old$b[5:6] <- 0

pmat_new <- data.frame(date = idat$date)
pmat_new$a <- 1
pmat_new$b <- c(1, 1, 1, 1, 0)
pmat_new$c <- 1
pmat_new$d <- 1
pmat_new$e <- c(0, 0, 0, 1, 1)
```

```{r, echo=FALSE, eval = FALSE}
knitr::kable(idat, caption = "5 interactions between 6 individuals, observed on 4 days.")
```

```{r, echo=FALSE, results='asis'}
print(xtable(idat, caption = "5 interactions between 6 individuals, observed on 4 days.\\label{tab:tab_seq_toy}", digits = 0), caption.placement = "top", comment = FALSE, include.rownames = FALSE)
```


\begin{table}[!htb]
    \begin{minipage}{.5\linewidth}
      \centering
      \caption{One row per interaction.}
      \label{tab:tab_new}

```{r, echo=FALSE, results='asis'}
print(xtable(pmat_new, digits = 0), caption.placement = "top", comment = FALSE, include.rownames = FALSE, floating = FALSE)
```

    \end{minipage}%
    \begin{minipage}{.5\linewidth}
      \centering
        \caption{One row per date.}
        \label{tab:tab_old}

```{r, echo=FALSE, results='asis'}
print(xtable(pmat_old, digits = 0), caption.placement = "top", comment = FALSE, include.rownames = FALSE, floating = FALSE)
```

    \end{minipage} 
\end{table}


```{r, echo=FALSE, eval = FALSE}
knitr::kables(list(knitr::kable(pmat_new, caption = "one row per interaction"), 
                   knitr::kable(pmat_old, caption = "one row per date")))
```


And it is really important that this matrix has column names that correspond to the individuals:
All winners and losers must occur as column names!
The order of the columns does not matter and there can be additional columns, no problem, but all the ids need to be there.

If you have a working presence matrix formatted for `EloRating`, you can use the function `convert_presence()` to convert it for usage with `EloRating.Bayes`.

## a worked example

We start with a small simulated example.

```{r}
set.seed(123)
x <- EloRating::randomsequence(nID = 6, avgIA = 13, presence = c(0.7, 0.7))
winner <- x$seqdat$winner
loser <- x$seqdat$loser
Date <- x$seqdat$Date
presence <- x$pres
```


The first step is to convert the interaction (and presence) data into a format that can be passed to Stan (which is what runs under the hood).
The function `prep_seq()` does that and requires the three mandatory vectors (winner, loser, date) and optionally the matrix with the presence data.\footnote{\textit{Optional} additional data that can be supplied reflects whether interactions ended in a draw or not, and to distinguish different interaction types, which allows estimating different $k$ values.}
If you don't supply a presence matrix, the function will assume that all individuals were present for all dates.


```{r}
standat <- prep_seq(winner = winner, loser = loser, Date = Date, presence = presence)
```

`standat` is a list, the content of which you probably won't need to touch.

With this done, we can fit the Elo model, using 

```{r, eval=TRUE, echo=FALSE}
res <- elo_seq_bayes(standat = standat, quiet = TRUE, parallel_chains = 4, seed = 1)
```

```{r, eval=FALSE}
res <- elo_seq_bayes(standat = standat, seed = 1)
```

So we only need to pass the `standata`. 
All other arguments in this function relate to `$sample()` from `cmdstanr`, and are optional. 
For example, if you want use parallel processing, you can set `parallel_chains = 4`, or as in the case above, I used a fixed `seed`.
You can also change the number of chains (default is 4), iterations (default is 1,000 each for warmup and sampling), etc (see the help file `elo_seq_bayes` for the most common ones).

That's it.
Now, we can inspect the results.

```{r}
summary(res)
```

### working with the model results

Working with the results of such a model requires a little planning/thinking.
The reason is that all the functions related to extracting/plotting the actual ratings require new computations.
One could theoretically compute the ratings for each individual and for each date in the function, and store them in the output object.
But that would lead to huge objects (and files), depending on the number of interactions and individuals.
[This is much less a problem in the classic package, where there is essentially one rating per day, not several thousand posterior samples for each rating.]
So for now I decided to go the way of keeping the workload on the post-fitting side.
Maybe in the future I might add an argument that allows doing *most* of the heavy lifting in `elo_seq_bayes`, which then might speed up downstream steps.



We can extract the ratings numerically, using `extract_elo_b()`.
There are three major decisions to be made when using the function.

(1) Ratings are required for what date(s)? 
This is handled via the argument `targetdate`. 
By default, the ratings of the day of the last interaction are returned.

(2) Do you want point estimates or the full posteriors?
This is handled via the argument `make_summary`.
The default provides the summary, i.e. the point estimate.

(3) Do you want any individuals that were not present on the specified date(s) be included in the output?
This is handled via the argument `keep_absent`.
By default, the output includes such individuals, and its/their values are set to `NA`.

One additional argument I use here is `quiet = TRUE`, which just prevents output that `cmdstanr` normally prints to the console is *not* displayed/printed to the console.

So the easiest version is to get point estimates (alongside some measures of uncertainty) for the date of the last interaction.
Since individual *m* was not present, its values are returned as `NA`.
The point estimates (posterior median and mean) are probably the first thing to look at here.

```{r}
extract_elo_b(res, quiet = TRUE)
```

If you want to return ratings for more than one date, we use `targetdate`.
Incidentally, only two individuals were present on the first date, which leads to the pretty empty looking table.

```{r}
extract_elo_b(res, targetdate = c("2000-01-30", "2000-02-08"), quiet = TRUE)
```

If you want to go for the full posteriors, things are a little more complex, but it's worth the effort.
The output in that case is a list, where each list item corresponds to one entry in `targetdate`.
Each list item is a matrix with one column per individual.
By default the absent individuals are represented as columns with `NA` values.
Since with the default sampling settings, we obtain 4,000 posterior samples, we only display the first 6 rows here.


```{r}
rats <- extract_elo_b(res, targetdate = c("2000-01-30", "2000-02-08"), make_summary = FALSE)
lapply(rats, head)
```

And just to confirm, the posteriors can be 'converted' into the values we obtained when using `make_summary = TRUE`.

```{r}
lapply(rats, colMeans)
```



Visually.

All still very experimental...

```{r, echo=2:100, fig.width=8, fig.height=3, out.width="90%", fig.cap="Posterior rating distribution for seven individuals."}
par(family = "serif", mgp = c(1.5, 0.4, 0), mar = c(3, 2.6, 3, 1),  tcl = -0.2, las = 1, cex.axis = 0.7)
plot_scores(res, color = FALSE)
```


The longitudinal version is here (and again, still a very early version!).
The caveat here is that the date are binned for display. 
So if there are gaps in the presence of an individual and during one of its stints, an individual is only represent in one bin, it will appear as a circle.

If you want to plot daily ratings you can increase the resolution (`resol=`) such that each bin reflects one day.
But note that then the computations might require a substantial amount of time.

```{r, echo=2:100, fig.width=7, fig.height=3, out.width="80%", fig.cap="Longitudinal display of ratings with 89\\% credible interval."}
par(family = "serif", mgp = c(1.5, 0.4, 0), mar = c(3, 2.6, 3, 1),  tcl = -0.2, las = 1, cex.axis = 0.7)
plot_scores_longitudinal(res, unc_width = 0.89, resol = 16)
```



\clearpage

# intuitions

This section is just a place to collect illustrations of intuitions I have/had about the modelling of ratings.

## individuals with less interactions have more uncertain ratings


```{r}
set.seed(1)
x <- EloRating::randomsequence(nID = 8, avgIA = 50)$seqdat
all_ids <- unique(c(x$winner, x$loser))

# make one individual 'rare', i.e. with relatively few interactions
excl <- which(x$winner == all_ids[1] | x$loser == all_ids[1])
excl <- sample(excl, length(excl) * 0.95)
xdata <- x[-excl, ]
table(c(xdata$winner, xdata$loser))
```

```{r, echo=FALSE}
ncores <- 1
if (parallel::detectCores() >= 4) ncores <- 4
d <- prep_seq(xdata$winner, xdata$loser, xdata$Date)
res <- elo_seq_bayes(standat = d, quiet = TRUE, parallel_chains = ncores)
```

```{r, eval=FALSE}
d <- prep_seq(xdata$winner, xdata$loser, xdata$Date)
res <- elo_seq_bayes(standat = d, quiet = TRUE)
```

```{r, echo = 2:100, fig.width=7, fig.height=2.5, out.width="60%", fig.cap="Posterior ratings. Note the wide posterior of individual i, which is partly due to it being only observed in three interactions."}
par(family = "serif", mgp = c(1.5, 0.4, 0), mar = c(3, 2.6, 3, 1),  tcl = -0.2, las = 1, cex.axis = 0.7)
plot_scores(res)
```

```{r, echo = 3:11, fig.width=7, fig.height=2.5, out.width="80%", fig.cap="Number of interactions and width of rating posterior at the end of the sequence and at the beginning (start ratings)."}
par(family = "serif", mgp = c(1.5, 0.4, 0), mar = c(3, 2.6, 3, 1),  tcl = -0.2, las = 1, cex.axis = 0.7)
par(mfrow = c(1, 2))
e <- extract_elo_b(res)
plot(as.numeric(table(c(xdata$winner, xdata$loser))[e$id]), e$mad, 
     xlab = "interactions", ylab = "width of posterior")
title(main = "final ratings")

e <- extract_elo_b(res, targetdate = 0)
plot(as.numeric(table(c(xdata$winner, xdata$loser))[e$id]), e$mad, 
     xlab = "interactions", ylab = "width of posterior")
title(main = "start ratings")
```


\clearpage

# Bayesian versus classic comparison

For the fun of it: let's compare the performance between the classic and the Bayesian version with a simulated and a real world example.


```{r sim_example, cache=TRUE}
set.seed(123)
# generate data
x <- EloRating::randomsequence(nID = 10, avgIA = 100, presence = c(0.4, 0.6))

# classic elo
celo <- EloRating::elo.seq(winner = x$seqdat$winner, loser = x$seqdat$loser,
                           Date = x$seqdat$Date, presence = x$pres)

# Bayesian elo
standat <- prep_seq(winner = x$seqdat$winner, loser = x$seqdat$loser,
                    Date = x$seqdat$Date, presence = x$pres)
belo <- elo_seq_bayes(standat = standat, refresh = 0, parallel_chains = 4, 
                      seed = 123, quiet = TRUE)
```


```{r sim_example_ratings}
# belo final ratings
brats1 <- extract_elo_b(belo, keep_absent = FALSE)
# celo final ratings (order matched to order of brats)
crats1 <- EloRating::extract_elo(celo)[as.character(brats1$id)]

# ratings halfway through the sequence
hdate <- names(standat$idates)[length(standat$idates)/2]
# belo half-way ratings
brats2 <- extract_elo_b(belo, targetdate = hdate, keep_absent = FALSE)
# celo half-way ratings (order matched to order of brats)
crats2 <- EloRating::extract_elo(celo, extractdate = hdate)[as.character(brats2$id)]
```

Here is the plot that compares ratings half-way through the interaction sequence and at the end of the sequence using the classic version and the Bayesian version.

```{r, fig.width=8, fig.height=3, out.width="100%", echo=2:6}
par(mfrow = c(1, 2), mgp = c(1.5, 0.4, 0), mar = c(3, 2.6, 3, 1), tcl = -0.2, las = 1, cex.axis = 0.8, family = "serif", las = 1)
plot(crats2, brats2$median, xlim = range(crats1), ylim = range(brats1$median), ann = FALSE)
title(xlab = "classic", ylab = "Bayesian (post. median)", main = "half-way")

plot(crats1, brats1$median, xlim = range(crats1), ylim = range(brats1$median), ann = FALSE)
title(xlab = "classic", ylab = "Bayesian (post. median)", main = "final")
```


The longitudinal plot for the Bayesian version is still experimental!
For comparison, I added the results from the classic version as well.

```{r sim_example_longifig, fig.width=8, fig.height=3, out.width="100%", echo=2:50}
par(mfrow = c(1, 2), family = "serif", mgp = c(1.5, 0.4, 0), mar = c(3, 2.6, 3, 1), tcl = -0.2, las = 1, cex.axis = 0.7)

plot_scores_longitudinal(belo, resol = 15)
title(main = "Bayesian")
box()

ratings <- celo$cmat
dates <- celo$truedates

plot(0, 0, xlim = range(dates), ylim = range(ratings, na.rm = TRUE),
     axes = FALSE, xlab = "Date",
     ylab = "Elo-ratings")
for(i in 1:ncol(ratings)) points(dates, ratings[, i], type = "l") 
axis.Date(1, x = dates)
axis(2, las = 1)
box()

title(main = "classic")
```

# steepness (longitudinal edition)

This section (and the corresponding functions) should be considered experimental!

The idea is to track potential changes in steepness in time directly from the model that estimates the ratings itself.
What I mean by that can be demonstrated most easily with a simulated data set of an interaction sequence.
We generate a data set where the dominance system initially has a steepness of 0.9.
Then there is a time point from which on the system has only a steepness of 0.2 (imagine things like the onset of the birth season, or the rain season, some social perturbation (group split, infanticide, anything), or an experimental manipulation).
This then is followed again by a period with higher steepness (0.6).
So we have three periods, with different steepness.
For the simulation we *know* the periods (in reality we won't, but for the sake of prooving the concept this is necessary).

So let's simulate some data.
We have 12 individuals and in each of the three periods, there are 500 interactions.
We also extract the three dates at which we want to evaluate the steepness (which correspond to the last day in each period).

```{r longi_steep_data, cache=TRUE}
set.seed(123)
idata <- generate_interactions(n_ind = 12, n_int = c(500, 500, 500), steep = c(0.9, 0.2, 0.6))
dates <- c(idata$date[max(which(idata$set == 1))], 
           idata$date[max(which(idata$set == 2))], 
           idata$date[max(which(idata$set == 3))])
```

Then we fit the model.
This generates a warning, which I ignore for now (I'm fairly sure that coding the individuals as numbers works correctly, but I still have to convince myself to 100% certainty about that).

```{r longi_steep_model, cache=TRUE}
res <- elo_seq_bayes(prep_seq(winner = idata$winner, loser = idata$loser, Date = idata$date), 
                     parallel_chains = 4, quiet = TRUE)
```

Now we can plot the ratings over time.
We add lines for the three dates.
It seems fairly obvious from that figure that the spread of the ratings roughly reflects the three periods (more spread = steeper, less spread = less steep).

```{r longisteepplotscores, cache=TRUE, fig.width=6, fig.height=3, out.width="70%", echo=2:50, fig.align='center', fig.cap="Individual scores over time when steepness changes."}
par(family = "serif", mgp = c(1.5, 0.4, 0), mar = c(3, 2.6, 3, 1), tcl = -0.2, las = 1, cex.axis = 0.7)
plot_scores_longitudinal(res, resol = 13)
abline(v = dates)
```

Now we can extract the steepness at each of three time points numerically.
This results in a matrix with three columns (one per date of interest) and the values are the posterior samples for the steepness at each time point.
The column means should come close to values we set up in the data generation.

```{r longi_steep_post_steeps}
r <- longitudinal_steepness(res = res, dates = dates)
round(colMeans(r), 2)
```

And finally, there is also a function that visualizes the three posteriors along a time axis.

```{r longisteepplotsteepnessposteriors, cache=TRUE, fig.width=6, fig.height=3, out.width="70%", echo=2:50, fig.align='center', fig.cap="Steepness over time when steepness changes. Shown are the three posteriors for the three steepness corresponding to three dates."}
par(family = "serif", mgp = c(1.5, 0.4, 0), mar = c(3, 2.6, 3, 1), tcl = -0.2, las = 1, cex.axis = 0.7)
plot_steepness_longitudinal(res = res, dates = dates)
```

As said initially, this is still very experimental.
In any case, in this example we recover the input steepness values for the three time periods fairly well.
That said, this should be considered only a proof of concept.
We had a lot of interaction data here (500 interactions per period) and there are no demographic fluctuations (there were the same 12 individuals from beginning to end).

What would be interesting to know now (in addition to play with interaction numbers, migration events, and more periods) is how the approach compares to what would happen if we actually modeled the steepness values for each period separately (i.e., fit three steepness models with 500 interactions each).
But I leave that for later.


